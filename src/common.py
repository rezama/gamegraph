'''
Created on Dec 11, 2011

@author: reza
'''
import getopt
import gzip
import os
import random
import sys

import numpy as np
from pybrain.structure.modules.sigmoidlayer import SigmoidLayer  # pylint: disable=import-error
from pybrain.tools.shortcuts import buildNetwork  # pylint: disable=import-error

from params import (ALTERNATE_SEATS, EVAL_OPPONENT, EVAL_OPPONENT_OPTIMAL,
                    EVAL_OPPONENT_RANDOM, EVAL_OPPONENT_SARSA,
                    GAMESET_PROGRESS_REPORT_EVERY_N_GAMES,
                    GAMESET_PROGRESS_REPORT_USE_GZIP,
                    GAMESET_RECENT_WINNERS_LIST_SIZE, MAX_MOVES_PER_GAME,
                    PRINT_GAME_DETAIL, PRINT_GAME_RESULTS, SAVE_STATS,
                    TRAIN_BUDDY, TRAIN_BUDDY_SELF, USE_SEEDS)

PLAYER_WHITE = 0
PLAYER_BLACK = 1

PLAYER_NAME = {}
PLAYER_NAME[PLAYER_WHITE] = 'White'
PLAYER_NAME[PLAYER_BLACK] = 'Black'

# Note: These literals are used in application logic, using expressions like:
# if state_str.startswith(PLAYER_NAME_SHORT[PLAYER_WHITE])...
PLAYER_NAME_SHORT = {}
PLAYER_NAME_SHORT[PLAYER_WHITE] = 'w'
PLAYER_NAME_SHORT[PLAYER_BLACK] = 'b'


def other_player(player):
    return 1 - player


REWARD_WIN = 1.0
REWARD_LOSE = 0.0

# EXP_BASE = 'base'
EXP_P = 'p'
EXP_OFFSET = 'offset'
EXP_GRAPH = 'graph'

DEFAULT_DOMAIN_NAME = None
DEFAULT_EXP = None
DEFAULT_P = 1.0
DEFAULT_OFFSET = 0
DEFAULT_GRAPH_NAME = None
DEFAULT_CHOOSE_ROLL = 0.0
DEFAULT_TRIAL = 0

POS_ATTR = 'pos'
DIST_ATTR = 'd'
BFS_COLOR_ATTR = 'bfscolor'
VAL_ATTR = 'value'
VOLATILITY_ATTR = 'volatility'

# FOLDERS where data files generated by the program are stored.
FOLDER_TRIALS = '../data/trials'
FOLDER_AVG = '../data/avg'
FOLDER_DOMAINSTATS = '../data/domainstats'
FOLDER_GRAPH = '../data/graph'
SUFFIX_GRAPH_LOCK = '-lock'
FOLDER_QTABLE = '../data/q-table'
FOLDER_PLOTS = '../data/plots'

# Folders containing scripts.
FOLDER_CONDOR = '../condor'  # Contains condor scripts.
FILE_CONDOR_PLAN = FOLDER_CONDOR + '/plan.txt'  # This file is generated by experiment_planner.py.
FOLDER_PLOT = '../plot' # Contains gnuplot scripts.
FILE_PLOT_PLAN = FOLDER_PLOT + '/plan.gp'  # This file is generated by experiment_planner.py.

FILE_PREFIX_SARSA = 'sarsa'
FILE_PREFIX_NTD = 'ntd'
FILE_PREFIX_HC = 'hc'
FILE_PREFIX_HC_CHALLENGE = 'hc-challenge'


def make_data_folders():
    for path in [FOLDER_TRIALS, FOLDER_AVG, FOLDER_DOMAINSTATS, FOLDER_GRAPH,
                 FOLDER_QTABLE, FOLDER_PLOTS]:
        try:
            os.makedirs(path)
        except OSError:  # Already exists.
            pass

class Die(object):

    def __init__(self, num_sides):
        self.num_sides = num_sides
        self.sides = range(1, num_sides + 1)

    def roll(self):
        return random.choice(self.sides)

    def get_all_sides(self):
        return self.sides


class Action(object):

    def __init__(self, num_checkers):
        self.num_checkers = num_checkers
        self.all_checkers = range(num_checkers)
        self.action_forfeit_move = num_checkers
        self.all_actions = self.all_checkers + [self.action_forfeit_move]

    def get_checker_name(self, i):
        if i == self.action_forfeit_move:
            return 'no checkers (forfeits move)'
        else:
            return 'Checker %d' % (i + 1)

    def random_action(self, state):
        action = self.action_forfeit_move
        checker = random.choice(self.all_checkers)
        tries_left = self.num_checkers
        while tries_left > 0:
            move_outcome = state.get_move_outcome(checker)
            if move_outcome is not None:
                return checker
            else:
                checker = self.next_checker(checker)
            tries_left -= 1

        return action

    def next_checker(self, checker):
        return (checker + 1) % (self.num_checkers)

    def get_num_checkers(self):
        return self.num_checkers

    def get_all_checkers(self):
        return self.all_checkers

    def get_all_actions(self):
        return self.all_actions


class Agent(object):

    def __init__(self, state_class):
        self.state_class = state_class
        self.state = None

    def set_state(self, state):
        self.state = state

    def begin_episode(self):
        pass

    def end_episode(self, reward):
        pass

    def select_action(self):
        raise NotImplementedError

    def __str__(self):
        return self.__class__.__name__


class AgentNeural(Agent):

    def __init__(self, state_class, outputdim, init_weights=None):
        super(AgentNeural, self).__init__(state_class)
        self.inputdim = self.state_class.get_network_inputdim()
        self.hiddendim = self.state_class.get_network_hiddendim()
        self.outputdim = outputdim
        self.network = buildNetwork(self.inputdim, self.hiddendim,
                                    self.outputdim, hiddenclass=SigmoidLayer,
                                    bias=True)
        if init_weights is not None:
            self.network.params[:] = [init_weights] * len(self.network.params)
        self.astar_value = {}

    def get_Q_value(self, state, action):
        raise NotImplementedError

    def get_scalar_Q_value(self, state, action):
        raise NotImplementedError

    def select_action_super(self, choose_random_action):
        can_choose_roll = (True if self.state.stochastic_p < self.state.exp_params.choose_roll
                           else False)

        if TRAIN_BUDDY == TRAIN_BUDDY_SELF:
            # If playing for white, pick the action with highest reward for white.
            # If playing for black, pick the action with lowest reward for white.
            reverse = self.state.player_to_move == PLAYER_WHITE
        else:
            # When TRAIN_BUDDY != TRAIN_BUDDY_SELF, the agent maintains
            # state values for white and black players from their own
            # perspectives.  So, regardless of player's color, we need
            # to pick actions leading to states with higher values.
            reverse = True

        all_rolls = self.state.die_object.get_all_sides()
        avail_rolls = all_rolls if can_choose_roll else [self.state.roll]

        action_values = []  # For all avail_rolls: (roll, action) -> value.
        roll_values = [None] * len(all_rolls)  # For all rolls: roll -> value.
        scalar_roll_values = [None] * len(all_rolls)
        for replace_roll in all_rolls:
            roll_index = replace_roll - 1
            self.state.roll = replace_roll
            for action in self.state.action_object.get_all_checkers():
                move_outcome = self.state.get_move_outcome(action)
                if move_outcome is not None:
                    move_value = self.get_Q_value(self.state, action)
                    scalar_move_value = self.get_scalar_Q_value(self.state, action)
                    if roll_values[roll_index] is None:
                        roll_values[roll_index] = move_value
                        scalar_roll_values[roll_index] = scalar_move_value
                    else:
                        if reverse:  # We want the action with highest value.
                            if scalar_move_value > scalar_roll_values[roll_index]:
                                scalar_roll_values[roll_index] = scalar_move_value
                                roll_values[roll_index] = move_value
                        else:
                            if scalar_move_value < scalar_roll_values[roll_index]:
                                scalar_roll_values[roll_index] = scalar_move_value
                                roll_values[roll_index] = move_value
                    # Consider this roll-action for choosing the best action
                    # only if the associated roll is available to the agent.
                    if replace_roll in avail_rolls:
                        # insert a random number to break the ties
                        action_values.append(((scalar_move_value, random.random()),
                                              (replace_roll, action)))

        # Compute max_b Q(s, b), for undetermined roll, under current
        # choose_roll probability.
        # First have to remove Nones for rolls that are not legal.
        legal_roll_values = [v for v in roll_values if v is not None]
        legal_scalar_roll_values = [v for v in scalar_roll_values if v is not None]
        avg_roll_value = np.mean(np.array(legal_roll_values), axis=0)
        preferred_roll_value = (max(legal_scalar_roll_values) if reverse
                                else min(legal_scalar_roll_values))
        preferred_roll_index = legal_scalar_roll_values.index(preferred_roll_value)
        value_with_choose_roll = (self.state.exp_params.choose_roll *
                                  legal_roll_values[preferred_roll_index])
        value_with_random_roll = (1 - self.state.exp_params.choose_roll) * avg_roll_value
        astar_value = value_with_random_roll + value_with_choose_roll
        state_str_no_roll = str(self.state)[:-2]
        self.astar_value[state_str_no_roll] = astar_value

        # Select best action.
        if action_values:  # len(action_values) > 0:
            action_values_sorted = sorted(action_values, reverse=reverse)
            if choose_random_action:
                index = random.randint(0, len(action_values) - 1)
            else:
                index = 0
            best_roll = action_values_sorted[index][1][0]
            # set the best roll there
            self.state.roll = best_roll
            action = action_values_sorted[index][1][1]
        else:
            action = self.state.action_object.action_forfeit_move
            # Cache network values for action_forfeit_move, since the
            # calculations for astar_value[...] never fetch
            # Q(s, action_forfeit_move).
            self.get_Q_value(self.state, action)

        return action

#    def __repr__(self):
#        return str(self.network.params)


class Game(object):

    def __init__(self, exp_params, game_number, agent_white, agent_black,
                 player_to_start_game):
        self.game_number = game_number
        self.agents = [None, None]
        self.agents[PLAYER_WHITE] = agent_white
        self.agents[PLAYER_BLACK] = agent_black
        self.exp_params = exp_params
        self.state = exp_params.state_class(exp_params, player_to_start_game)

        # initial die roll
        self.state.roll = self.state.die_object.roll()
        self.state.stochastic_p = random.random()

        agent_white.set_state(self.state)
        agent_black.set_state(self.state)
        self.count_plies = 0

    def play(self):
        while (not self.state.is_final() and
               (self.count_plies < MAX_MOVES_PER_GAME * 2)):
            # if self.player_to_play == PLAYER_WHITE:
            #     self.state.compute_per_ply_stats(self.count_plies)
            self.state.compute_per_ply_stats(self.count_plies)
            if PRINT_GAME_DETAIL:
                self.state.print_state()
            done = False
            while not done:
                action = self.agents[self.state.player_to_move].select_action()
                if PRINT_GAME_DETAIL:
                    print '#  %s rolls %d, playing %s...' % \
                            (PLAYER_NAME[self.state.player_to_move],
                             self.state.roll,
                             self.state.action_object.get_checker_name(action))
                success = self.state.move(action)
                if PRINT_GAME_DETAIL:
                    print '# '
                if success:
                    done = True
                else:
                    self.state.reroll_dice()

            self.count_plies += 1

        if PRINT_GAME_DETAIL:
            self.state.print_state()

        self.state.compute_per_game_stats(self.game_number)

        winner = None
        loser = None
        if self.state.has_player_won(PLAYER_WHITE):
            winner = PLAYER_WHITE
            loser = PLAYER_BLACK
        elif self.state.has_player_won(PLAYER_BLACK):
            winner = PLAYER_BLACK
            loser = PLAYER_WHITE
        else:
            print 'Warning: game %d took too long, declaring %s winner' % (
                self.game_number, self.agents[PLAYER_BLACK])
            winner = PLAYER_BLACK
            loser = PLAYER_WHITE

        self.agents[winner].end_episode(REWARD_WIN)
        self.agents[loser].end_episode(REWARD_LOSE)

        return winner

    def get_count_plies(self):
        return self.count_plies

    @classmethod
    def get_max_episode_reward(cls):
        return REWARD_WIN

    @classmethod
    def get_min_episode_reward(cls):
        return REWARD_LOSE


class GameSet(object):

    def __init__(self, exp_params, num_games, agent1, agent2,
                 print_learning_progress=False, progress_filename=None):
        self.num_games = num_games
        self.agent1 = agent1
        self.agent2 = agent2
        self.exp_params = exp_params
        self.print_learning_progress = print_learning_progress
        self.progress_filename = progress_filename

        self.sum_count_plies = 0

    def run(self):
        if self.progress_filename is not None:
            if GAMESET_PROGRESS_REPORT_USE_GZIP:
                f = gzip.open(self.progress_filename + '.gz', 'w')
            else:
                f = open(self.progress_filename, 'w')

        game_series_size = 1
        if ALTERNATE_SEATS:
            game_series_size *= 2

        if USE_SEEDS:
            random_seeds = []
            for _ in range(self.num_games / game_series_size):
                random_seeds.append(random.random())

        # agent1, agent2
        players = [self.agent1, self.agent2]
        seats_reversed = False
        count_wins = [0, 0]
        wins_by_color = [0, 0]
        recent_winners = []  # 0 for agent1, 1 for agent2

        player_to_start_game = PLAYER_WHITE
        for game_number in range(self.num_games):
            if ALTERNATE_SEATS:
                # if game_number % game_series_size == 0:
                players[:] = [players[1], players[0]]
                seats_reversed = not seats_reversed
            # load random seed
            if USE_SEEDS:
                random.seed(random_seeds[game_number / game_series_size])
            # setup game
            players[0].begin_episode()
            players[1].begin_episode()
            if PRINT_GAME_RESULTS:
                print 'Starting game %2d between %s (%s) and %s (%s)' % (
                    game_number, str(players[0]), PLAYER_NAME[PLAYER_WHITE],
                    str(players[1]), PLAYER_NAME[PLAYER_BLACK])
            game = Game(self.exp_params, game_number,
                        players[0], players[1], player_to_start_game)
            winner = game.play()  # 0 for white, 1 for black.
            wins_by_color[winner] += 1
            if seats_reversed:
                winner_agent = other_player(winner)
            else:
                winner_agent = winner
            count_wins[winner_agent] += 1
            if self.print_learning_progress:
                if len(recent_winners) > GAMESET_RECENT_WINNERS_LIST_SIZE - 1:
                    recent_winners.pop(0)
                recent_winners.append(winner_agent)
            if PRINT_GAME_RESULTS:
                print 'Game %2d won by %s playing as %s in %2d plies' % (
                    game_number, str(players[winner]), PLAYER_NAME[winner],
                    game.count_plies)
            if self.print_learning_progress:
                if game_number % GAMESET_PROGRESS_REPORT_EVERY_N_GAMES == 0:
                    win_rate = float(recent_winners.count(0)) / len(recent_winners)
                    print 'Played game %2d, recent win rate: %.2f' % (
                        game_number, win_rate)
                    if self.progress_filename is not None:
                        f.write('%d %f\n' % (game_number, win_rate))
            self.sum_count_plies += game.get_count_plies()
#            player_to_start_game = other_player(player_to_start_game)

        if self.progress_filename is not None:
            f.close()

        return count_wins + wins_by_color

    def get_sum_count_plies(self):
        return self.sum_count_plies


class ExpParams(object):

    exp_param_cached = None

    def __init__(self, domain_name, exp, graph_name, p, offset, choose_roll,
                 trial, exp_signature):
        self.domain_name = domain_name
        # I believe this import is here to avoid a circular dependency.
        from domain_proxy import DomainProxy
        self.state_class = DomainProxy.load_domain_state_class_by_name(domain_name)
        self.exp = exp
        self.graph_name = graph_name
        self.p = p
        self.offset = offset
        self.choose_roll = choose_roll
        self.trial = trial
        self.exp_signature = exp_signature

        self.domain_signature = self.state_class.get_domain_signature()
        self.signature = self.domain_signature
        if self.exp_signature != '':
            self.signature = self.domain_signature + '-' + self.exp_signature

    @classmethod
    def get_exp_params_from_command_line_args(cls):
        if cls.exp_param_cached is None:
            cls.exp_param_cached = cls.get_exp_params(sys.argv[1:])

        return cls.exp_param_cached

    @classmethod
    def get_exp_params(cls, options_list):
        domain_name = DEFAULT_DOMAIN_NAME
        exp = DEFAULT_EXP
        graph_name = DEFAULT_GRAPH_NAME
        p = DEFAULT_P
        offset = DEFAULT_OFFSET
        choose_roll = DEFAULT_CHOOSE_ROLL
        trial = DEFAULT_TRIAL

        options, remainder = getopt.getopt(  # pylint: disable=unused-variable
            options_list, 'd:g:o:p:c:t:',
            ['domain=', 'graph=', 'offset=', 'p=', 'chooseroll=', 'trial='])

        exp_signature = ''

        for opt, arg in options:
            if opt in ('-d', '--domain'):
                print 'Setting domain to: %s' % arg
                domain_name = arg
                # exp_signature += arg + '-'
            elif opt in ('-g', '--graph'):
                print 'Setting graph name to: %s' % arg
                graph_name = arg
                exp_signature += 'graph-%s-' % arg
            elif opt in ('-p', '--p'):
                print 'Setting p to: %s' % arg
                p = float(arg)
                exp_signature += 'p-%s-' % arg
            elif opt in ('-o', '--offset'):
                print 'Setting offset to: %s' % arg
                offset = int(arg)
                exp_signature += 'offset-%s-' % arg
            elif opt in ('-c', '--chooseroll'):
                print 'Setting choose_roll to: %s' % arg
                choose_roll = float(arg)
                exp_signature += 'chooseroll-%s-' % arg
            elif opt in ('-t', '--trial'):
                print 'Setting trial to: %s' % arg
                trial = int(arg)
                # exp_signature += 'trial-%s' % arg

        if exp_signature.rfind('-') >= 0:
            exp_signature = exp_signature[:exp_signature.rfind('-')]  # Remove last hyphen.

        if domain_name is None:
            # pylint: disable=line-too-long
            print 'Please specify an experiment using:'
            print ''
            print 'python %s --domain=<name> --graph=<name> [--choose-roll=<frac>] [--trial=<trial>]' % sys.argv[0]
            print 'python %s --domain=<name> --p=<frac> [--choose-roll=<frac>] [--trial=<trial>]' % sys.argv[0]
            print 'python %s --domain=<name> --offset=<int> [--choose-roll=<frac>] [--trial=<trial>]' % sys.argv[0]
            # pylint: enable=line-too-long
            sys.exit(-1)
        else:
            print 'Experiment signature is: %s' % exp_signature
            cls.exp_param_cached = ExpParams(domain_name, exp, graph_name,
                                             p, offset, choose_roll, trial,
                                             exp_signature)
            return cls.exp_param_cached

    def get_filename_suffix_with_trial(self):
        return self.get_filename_suffix_no_trial() + ('-%d' % self.trial)

    def get_filename_suffix_no_trial(self):
        return self.signature
        # if self.exp == EXP_BASE:
        #     return EXP_BASE
        # if self.exp == EXP_P:
        #     return '%s-%1.2f' % (self.exp, self.p)
        # elif self.exp == EXP_OFFSET:
        #     return '%s-%d' % (self.exp, self.offset)
        # elif self.exp == EXP_GRAPH:
        #     return '%s-%s' % (self.exp, self.graph_name)
        # else:
        #     return 'invalidexp'

    def get_custom_filename_no_trial(self, folder, file_prefix):
        filename = '%s/%s-%s.txt' % (folder, file_prefix,
                                     self.get_filename_suffix_no_trial())
        return filename

    def get_custom_filename_with_trial(self, folder, file_prefix):
        filename = '%s/%s-%s.txt' % (folder, file_prefix,
                                     self.get_filename_suffix_with_trial())
        return filename

    def get_trial_filename(self, file_prefix):
        filename = '%s/%s-%s.txt' % (FOLDER_TRIALS, file_prefix,
                                     self.get_filename_suffix_with_trial())
        return filename

    def get_graph_filename(self):
        if self.graph_name is not None:
            filename = '%s/%s' % (FOLDER_GRAPH, self.graph_name)
        else:
            filename = '%s/%s' % (FOLDER_GRAPH, self.get_filename_suffix_no_trial())
        return filename

    def is_graph_based(self):
        return self.graph_name is not None

    def is_first_trial(self):
        return self.trial == 0


class Experiment(object):

    @classmethod
    def create_eval_opponent_agent(cls, exp_params):
        agent_eval = None
        from app_sarsa import AgentSarsa
        from app_random_games import AgentRandom
        from app_optimal import AgentOptimal
        if EVAL_OPPONENT == EVAL_OPPONENT_RANDOM:
            agent_eval = AgentRandom(exp_params.state_class)
        elif EVAL_OPPONENT == EVAL_OPPONENT_SARSA:
            agent_eval = AgentSarsa(exp_params.state_class, load_knowledge=True)
        elif EVAL_OPPONENT == EVAL_OPPONENT_OPTIMAL:
            agent_eval = AgentOptimal(exp_params.state_class)
        return agent_eval

    @classmethod
    def save_stats(cls, state_class, exp_params):
        if not SAVE_STATS:
            return
        # # visit counts to individual states
        # filename = '../data/states-visit-count-%1.2f.txt' % state_class.p
        # f = open(filename, 'w')
        # for state, count_visits in sorted(state_class.states_visit_count.iteritems(),
        #                                   key=lambda (k, v): (v, k)):
        #     f.write('%s %d\n' % (state, count_visits))
        #     # print '%s: %d' % (state, count_visits)
        # f.close()

        # number of states discovered by game
        # filename = '../data/%s-%s-games-discovered-states-count.txt' % (
        #         domain.name, cls.get_filename_suffix_with_trial())
        filename = exp_params.get_custom_filename_with_trial(FOLDER_DOMAINSTATS,
                                                             'games-discovered-states-count')
        f = open(filename, 'w')
        for game_number in range(len(state_class.games_discovered_states_count)):
            f.write('%d %d\n' % (game_number,
                                 state_class.games_discovered_states_count[game_number]))
        f.close()

        # number of states discovered by game over average number of plies per game
        # filename = '../data/%s-%s-games-discovered-states-count-over-avg-num-plies.txt' % (
        #     domain.name, cls.get_filename_suffix_with_trial()))
        filename = exp_params.get_custom_filename_with_trial(
            FOLDER_DOMAINSTATS, 'games-discovered-states-count-over-avg-num-plies')
        f = open(filename, 'w')
        for game_number in range(len(state_class.games_discovered_states_count_over_avg_num_plies)):
            f.write('%d %f\n' % (
                game_number,
                state_class.games_discovered_states_count_over_avg_num_plies[game_number]))
        f.close()

        # filename = '../data/%s-%s-games-new-discovered-states-count.txt' % (
        #     domain.name, cls.get_filename_suffix_with_trial()))
        filename = exp_params.get_custom_filename_with_trial(
            FOLDER_DOMAINSTATS, 'games-new-discovered-states-count')
        f = open(filename, 'w')
        for game_number in range(len(state_class.games_discovered_states_count)):
            newly_discovered_count = state_class.games_discovered_states_count[game_number]
            if game_number != 0:
                newly_discovered_count -= state_class.games_discovered_states_count[game_number - 1]
            f.write('%d %d\n' % (game_number, newly_discovered_count))
        f.close()

        # number of visits to game states sorted by first ply of visit
        # filename = '../data/%s-%s-states-sorted-by-ply-visit-count.txt' % (
        #     domain.name, cls.get_filename_suffix_with_trial()))
        filename = exp_params.get_custom_filename_with_trial(
            FOLDER_DOMAINSTATS, 'states-sorted-by-ply-visit-count')
        f = open(filename, 'w')
        for i in range(len(state_class.states_sorted_by_ply_visit_count)):
            state_sorted_by_ply_visit_count = state_class.states_sorted_by_ply_visit_count[i]
            f.write('%d: %d\n' % (i, state_sorted_by_ply_visit_count))
        f.close()

        # same as above, over average num of plies per game
        # filename = '../data/%s-%s-states-sorted-by-ply-visit-count-over-avg-num-plies.txt' % (
        #     domain.name, cls.get_filename_suffix_with_trial()))
        filename = exp_params.get_custom_filename_with_trial(
            FOLDER_DOMAINSTATS, 'states-sorted-by-ply-visit-count-over-avg-num-plies')
        f = open(filename, 'w')
        for i in range(len(state_class.states_sorted_by_ply_visit_count_over_avg_num_plies)):
            state_sorted_by_ply_visit_count_over_avg_num_plies = (
                state_class.states_sorted_by_ply_visit_count_over_avg_num_plies[i])
            f.write('%d: %f\n' % (i, state_sorted_by_ply_visit_count_over_avg_num_plies))
        f.close()


class PrettyFloat(float):
    def __repr__(self):
        return "%+0.2f" % self
